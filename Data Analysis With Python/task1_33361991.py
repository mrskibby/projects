# -*- coding: utf-8 -*-
"""task1_33361991.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eW3auvSE6Usy6TbDqBO0kdrBxAAMWa91

<div class="alert alert-block alert-success">
    
# FIT5196 Task 1 in Assessment 1
#### Student Name: Nazmus Sakib
#### Student ID: 33361991

Date: 27th August 2023


Environment: Python 3.9

Libraries used:
* re (for regular expression, installed and imported)
* pandas (for data manipulation)
    
</div>

<div class="alert alert-block alert-danger">
    
## Table of Contents

</div>    

[1. Introduction](#Intro) <br>
[2. Importing Libraries](#libs) <br>
[3. Examining Patent Files](#examine) <br>
[4. Loading and Parsing Files](#load) <br>
$\;\;\;\;$[4.1. Defining Regular Expressions](#Reg_Exp) <br>
$\;\;\;\;$[4.2. Reading Files](#Read) <br>
$\;\;\;\;$[4.3. Whatever else](#latin) <br>
[5. Writing to CSV/JSON File](#write) <br>
$\;\;\;\;$[5.1. Verification - using the sample files](#test_xml) <br>
[6. Summary](#summary) <br>
[7. References](#Ref) <br>

-------------------------------------

<div class="alert alert-block alert-warning">

## 1.  Introduction  <a class="anchor" name="Intro"></a>
    
</div>

This assessment regards extracting data from semi-sctuctured text files. The dataset contained 500 `.txt` files which included various information about user reviews. In particular, ....

-------------------------------------

<div class="alert alert-block alert-warning">
    
## 2.  Importing Libraries  <a class="anchor" name="libs"></a>
 </div>

The packages to be used in this assessment are imported in the following. They are used to fulfill the following tasks:

* **re:** to define and use regular expressions
* **pandas:** ...
* ...
"""

import re
import pandas as pd

from google.colab import drive
drive.mount('/content/drive')

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 3.  Examining Raw Data <a class="anchor" name="examine"></a>

 </div>
"""

# List of file paths
file_paths = [
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input1.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input2.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input3.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input4.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input5.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input6.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input7.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input8.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input9.txt',
    '/content/drive/Shareddrives/FIT5196_S2_2023/Assessment 1/student_data/task1/33361991/33361991_task1_input10.txt'
]

# Initialize an empty list to store data
data = []

# Loop through each file and read its content
for file_path in file_paths:
    with open(file_path, "r") as file:
        content = file.readlines()  # Read the entire content of the file
        data.append(content)

# Initialize an empty string to store the combined content of all files
combined_content = ''

# Iterate through each file's lines and join them to the combined content
for lines in data:
    combined_content += ''.join(lines)

# Specify the output file path
output_file_path = 'combined_content.txt'

# Write the combined content to the output file
with open(output_file_path, "w") as output_file:
    output_file.write(combined_content)

print("Combined content written to:", output_file_path)

# Read and display the content of the saved file
with open(output_file_path, "r") as saved_file:
    saved_content = saved_file.read()
    # print(saved_content)

# Read and display the content of the saved file in chunks
chunk_size = 1000  # Define the number of characters to display at a time
start_index = 0

while start_index < len(saved_content):
    end_index = start_index + chunk_size
    print(saved_content[start_index:end_index])
    start_index = end_index

"""First of all, I have put all the input files path in a list and then loop thorugh each file path to read all the files and store in a text file named combined_content.txt. Then I display the contents using chunck_size so that there is no memory issue because the size is very big and there are 85000+ lines to display.

Having examined the file content, the following observations were made:
1. Each review starts with reviewer_id. So I assume that untill the start of next reviewer_ID the whole chuck belongs to 1 review.

2. The attributes are not named exactly the way it should be but the attributes start with '$' and ends with ':'.

3. There can be multiple attributes in one line of text.

4. There can also be no attributes in one line of text.

5. There can be empty lines where nothing is present and also it is not the start of another review.

-------------------------------------

<div class="alert alert-block alert-warning">

## 4.  Loading and Parsing Files <a class="anchor" name="load"></a>

</div>

<div class="alert alert-block alert-info">
    
### 4.1. Defining Regular Expressions <a class="anchor" name="Reg_Exp"></a>

In this section, the files are parsed and processed. First of all, appropriate regular expressions are defined to extract desired information when reading the files. The files content are extracted from the combined_content,txt as I have put everything in one txt file.

The appropriate regex is formated for every attribute needed and all of those reges is stored in a dictionary called patterns.

The rows that has all empty or none or null vales are removed as they are not needed.

Lastly the extracted data is stored in a dataframe named df with all the attributes.

Also the length of the df is checked to ensure that the data is extracted correctly

These patterns are used in the next step when reading the files.
"""

import re
import pandas as pd

# Read the content of the combined_content.txt file
with open("combined_content.txt", "r") as file:
    content = file.read()

# Define a regular expression pattern for each attribute
patterns = {
    "reviewer_id": r'\$(?:rev.*?ID|.*?ID|.*?rev)(?:\.|_)?\s*:\s*(.*?)$',
    "product_id": r'\$(?:pro.*?ID|.*?ID|.*?pro)(?:\.|_)?\s*:\s*(.*?)$',
    "reviewer_name": r'\$(?:rev.*?name|.*?name|.*?rev.*?name|.*?name.*?|.*?rev.*?Name)(?:\.|_)?\s*:\s*(.*?)$',
    "review_helpful": r'\$(?:rev.*?help|.*?help|.*?rev.*?helpful|.*?no.*?helps|.*?helpful)(?:\.|_)?\s*:\s*(.*?)$',
    "review_text": r'\$(?:rev.*?text|.*?text|.*?review.*?text|.*?text.*?|.*?rev.*?text)(?:\.|_)?\s*:\s*([\s\S]*?)(?=\$(?!(?:rev.*?ID|.*?ID|.*?rev)))',
    "review_summary": r'\$(?:rev.*?summary|.*?summary|.*?rev.*?SUMMARY|.*?SUMMARY.*?|.*?summary.*?)(?:\.|_)?\s*:\s*(.*?)$',
    "review_date": r'\$(?:rev.*?date|.*?date|.*?rev.*?DATE|.*?DATE.*?|.*?date.*?|.*?rev.*?Date)(?:\.|_)?\s*:\s*(.*?)$'
}

# Initialize a dictionary to store extracted data
data_dict = {attr: [] for attr in patterns.keys()}

# Extract data using the defined patterns
for attr, pattern in patterns.items():
    matches = re.findall(pattern, content, re.MULTILINE | re.IGNORECASE)
    data_dict[attr].extend(matches)

# Find the maximum length of extracted data
max_length = max(len(matches) for matches in data_dict.values())

# Extend shorter lists with None values to match the maximum length
for attr in data_dict.keys():
    data_dict[attr].extend([None] * (max_length - len(data_dict[attr])))

# Create a DataFrame from the extracted data
df = pd.DataFrame(data_dict)

# Remove rows with all NaN values
df.dropna(axis=0, how="all", inplace=True)

# Display the DataFrame
print(df.head(10))

print(len(df))

"""<div class="alert alert-block alert-info">
    
### 4.2. Reading Files <a class="anchor" name="Read"></a>

Now we can see that the columns reviwer_id and product_id are both containing the same value. And upon checking the original file it is figured that the odd numbered rows are the product id of the even numbered rows reviewer_id

So now what I do is take the even numbered rows of reviewer_id as the reviewer_id and the odd numbered rows as the product_id.

So for both the new columns are created to store thje correct value.

Lastly the length of the df is also checked so see if the length is approximately correct.
"""

# Create new DataFrames for reviewer_id and product_id
new_reviewer_df = df[df.index % 2 == 0].reset_index(drop=True)
new_product_df = df[df.index % 2 != 0].reset_index(drop=True)

# Remove the old product_id column and replace with new data
df = new_reviewer_df.rename(columns={'reviewer_id': 'reviewer_id'})

# Add the new product_id column
df['product_id'] = new_product_df['reviewer_id']

print(df.head(10))
print(len(df.reviewer_id))

"""Now We can see that the reviewer_id and product_id are correctly stored and the length of the df is also approximately correct

But if we check the reviwe_helpful column then it is also storing some arbitary values in that column.

So we split the column into 2 parts which is the actual review_helpful column and the rest part is named as review_helpful_date as it came from review helpful column.

As for reusability I have created another dataframe named df2 whoch is excatly same as df to work more efficiently.

So the first part of the review_helpful column is stored as original colummn and the rest is stored as date.
"""

df2 = df
# Define a function to extract values and date
def extract_review_data(text):
    try:
        helpful, rest = text.split('] ', 1)
        helpful = helpful[1:]
        date = rest.split(': ')[-1]
        return helpful, date
    except:
        return None, None

# Apply the function to the review_helpful column
df2[['review_helpful', 'review_helpful_date']] = df2['review_helpful'].apply(extract_review_data).apply(pd.Series)

# Clean up columns
df2['review_helpful'] = df2['review_helpful'].apply(lambda x: None if x is None else '[' + x + ']')

print(df2.head(10))

"""Now we can see that the columns are seprated correctly.

<div class="alert alert-block alert-info">
    
### 4.3. Processing Text <a class="anchor" name="latin"></a>

Just to check the columns I try to check for different value of review_date to check the consistancy.

I found out that the date with 12 11, 2012 has some problem.

To check I display all the data of review_date with the value 12 11, 2012
"""

rows_with_date = df2[df2['review_date'] == '12 11, 2012']

print(rows_with_date)

"""We can see that there is a problem with the reviewer_id and product_id.

The values are swaped for some of the records.

When I check the original files I can see that the values for both the columns are swaped.

So I swap the values of every reviewer_id value which are of 10 characters with product_id. This is because the product_id is 10 characters throughtout the combined_content and reviewer_id is whether 13 or 14 characters.
"""

for index, row in df2.iterrows():
    reviewer_id = row['reviewer_id']
    product_id = row['product_id']

    if reviewer_id and product_id:  # Check for non-None values
        if len(reviewer_id) == 10:  # Swap if length is 10
            df2.at[index, 'reviewer_id'], df2.at[index, 'product_id'] = product_id, reviewer_id

print(df2)

"""After the transformation I check the records again to make sure the values are stored correctly"""

rows_with_date = df2[df2['review_date'] == '12 11, 2012']

print(rows_with_date)

"""Now I create a regex to find the invalid rows with the date column. The date columns has data in 4 diffent kinds:

1. 11 12, 2012
2. 1 12, 2012
3. 01 1, 2012
4. 1 1, 2012

These are just for example.
"""

# Define the regular expression pattern with the additional formats
pattern = r'^(?:(?:0?[1-9]|1[0-2]) (?:0?[1-9]|[12][0-9]|3[01]), \d{4})$'

# Filter rows with values not matching the pattern for both columns
invalid_rows = df2[df2['review_helpful_date'].str.match(pattern, na=False) | df2['review_date'].str.match(pattern, na=False)]

print("Rows with values not matching the pattern:")
print(invalid_rows)

"""Now I adjust the review_date with review_helpful_date because the dates in review_helpful_date are more likely to be the correct values for review_dates.

I check if the review_date does not match the regex pattern, if not then check if the review_helpful_date matches the pattern. If yes then store the value of review_helpful_date to review_date.

if not then turn then into none cause we cannot have simple text in date column.
"""

# Define the regular expression pattern with the additional formats
pattern = r'^(?:(?:0?[1-9]|1[0-2]) (?:0?[1-9]|[12][0-9]|3[01]), \d{4})$'

# Function to update values based on conditions
def update_values(row):
    if isinstance(row['review_date'], str) and not re.match(pattern, row['review_date']):
        if isinstance(row['review_helpful_date'], str) and re.match(pattern, row['review_helpful_date']):
            row['review_date'] = row['review_helpful_date']
        else:
            row['review_date'] = None
    if isinstance(row['review_helpful_date'], str) and not re.match(pattern, row['review_helpful_date']):
        row['review_helpful_date'] = None
    return row

# Apply the custom function to the DataFrame
df2 = df2.apply(update_values, axis=1)

print("DataFrame after filtering and updating:")
print(df2)

"""Now I turn the review_date and review_helpful_date columns into date format(dd/mm/YYYY)"""

for col in ['review_helpful_date', 'review_date']:
    try:
        df2[col] = pd.to_datetime(df2[col].str.replace(',', ''), format='%m %d %Y').dt.strftime('%d/%m/%Y')
    except ValueError:
        df2[col] = None

print(df2)

"""-------------------------------------
Now remove all the rows that has none or null value for each and every column.
"""

# Remove rows with None or null values for all columns
df2_cleaned = df2.dropna(how='all')

# Print or use df2_cleaned as needed
print(df2_cleaned)

"""-------------------------------------
Now we can see that all the rows that has none or null values for all the columns has been removed. So only 6 rows were removed. Now we also tuern all the values that has 'None' to null values to keep consistancy
"""

# Replace 'None' values with None (null)
df2_cleaned_null = df2_cleaned.replace('None', None)

# Print or use df2_cleaned as needed
print(df2_cleaned_null)

"""In this step, all files are read and parsed.

Let's take a look at the first ten elements of the lists generated. We can see that ids, reviews,etc. are parsed and stored correctly.
"""

df2_cleaned_null.head(10)

"""Now drop the review_helpful_date column as it not needed. Also turn all the NaT values in review_date to null values"""

# Drop the 'review_helpful_date' column
df3 = df2_cleaned_null.drop(columns=['review_helpful_date'])
df3 = df3[df3['review_date'].notna()]

df3.head(10)

"""Now check the review_date column does it only contain one type of value. if yes then proceed ton xml."""

unique_helpful_values = df3['review_date'].apply(type).unique()
print(unique_helpful_values)

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 5.  Writing to XML File <a class="anchor" name="write"></a>

</div>

Now I will write the data into an xml file with the structure that is provided to us. I will also drop the column review_helpful_date column as it is not needed now.

This will also find the latest review date and show that first in the structure
"""

import xml.etree.ElementTree as ET

# Sort the DataFrame by reviewer_id and review_date
df3['review_date'] = pd.to_datetime(df3['review_date'], errors='coerce')
df3 = df3.sort_values(by=['reviewer_id', 'review_date'], ascending=[True, False])

# Create the XML structure
root = ET.Element('users')

users = {}  # Create an empty dictionary to store user elements

current_user = None
user_element = None
other_rows = []

# Loop through the DataFrame and create XML structure
for _, row in df3.iterrows():
    current_user = row['reviewer_id']
    current_product = row['product_id']

    # Skip rows with invalid review_date format or NaT values
    if not isinstance(row['review_date'], pd.Timestamp) or pd.isna(row['review_date']):
        other_rows.append(row)
        continue

    # Find or create the user element
    user_element = users.get(current_user)
    if user_element is None:
        user_element = ET.SubElement(root, 'user', id=current_user)
        latest_review_date_element = ET.SubElement(user_element, 'latest_review_date')
        latest_review_date_element.text = row['review_date'].strftime('%d/%m/%Y')

        reviews_element = ET.SubElement(user_element, 'reviews')
        users[current_user] = user_element

    # Create review element
    review_element = ET.SubElement(reviews_element, 'review')
    product_id_element = ET.SubElement(review_element, 'product_id')
    product_id_element.text = current_product if current_product else ""

    review_date_element = ET.SubElement(review_element, 'review_date')
    review_date_element.text = row['review_date'].strftime('%d/%m/%Y')

    review_helpful_element = ET.SubElement(review_element, 'review_helpful')
    review_helpful_element.text = row['review_helpful']

    review_text_element = ET.SubElement(review_element, 'review_text')
    review_text_element.text = row['review_text']

    review_summary_element = ET.SubElement(review_element, 'review_summary')
    review_summary_element.text = row['review_summary']

# Add the rows with invalid review_date format or NaT values at the end of the tree
for row in other_rows:
    user_element = users.get(row['reviewer_id'])
    if user_element is None:
        user_element = ET.SubElement(root, 'user', id=row['reviewer_id'])
        latest_review_date_element = ET.SubElement(user_element, 'latest_review_date')
        latest_review_date_element.text = ""

        reviews_element = ET.SubElement(user_element, 'reviews')
        users[row['reviewer_id']] = user_element

    other_row_element = ET.SubElement(reviews_element, 'other_row')
    for col in df2.columns:
        if col != 'reviewer_id':
            col_element = ET.SubElement(other_row_element, col)
            col_element.text = row[col]

# Create an ElementTree from the root element
tree = ET.ElementTree(root)

# Write the XML to a file
tree.write('33361991.xml', encoding='utf-8', xml_declaration=True)

"""-------------------------------------

<div class="alert alert-block alert-info">
    
### 5.1. Verification of the Generated XML File <a class="anchor" name="test_xml"></a>

To verify the XML file we check the root tag and the children of the tag
"""

# Load the XML file
tree = ET.parse('33361991.xml')
root = tree.getroot()

# Print the root tag and its children for first 20 elements
for index, child in enumerate(root[:20], start=1):
    print(f"{index}. {child.tag}: {child.attrib}")

"""-------------------------------------

<div class="alert alert-block alert-warning">

## 6. Summary <a class="anchor" name="summary"></a>

</div>

I started by explaining that I had a text file named "combined_content.txt" containing reviews. My goal was to extract specific attributes such as reviewer ID, product ID, reviewer name, review helpfulness, review text, review summary, and review date. To achieve this, I utilized regular expressions to capture the required information.

As I began implementing the provided Python script, I encountered some initial issues with data extraction. I continued working iteratively to refine the script, focusing on improving the regular expressions for each attribute. This involved accommodating the various naming variations and patterns they might have.

Despite the efforts, the script still didn't perform as expected. Some attributes were missing or incorrect, which prompted further debugging and fine-tuning of the regular expressions and extraction logic. I paid close attention to the formatting issues and variations in attribute names.

To handle the cases of missing values, I modified the script to store them as "None" within the DataFrame. This ensured that the extracted data would be properly structured and handled.

My attention then shifted to organizing the data into an XML format that grouped reviews by users. I refined the script to establish an XML structure where reviews were organized based on users and sorted by the latest review date. This required careful handling of "NaT" (Not a Timestamp) values, which indicated missing or invalid dates.

The final touch was generating the XML file named "user_reviews.xml." This file effectively structured the data as intended, with users' reviews appropriately organized and sorted.

Curious to verify the XML file's content, I sought to load and print the first 20 elements of the XML. This step allowed me to confirm that the XML structure aligned with my expectations, providing a visual validation of the achieved outcome.

Throughout this process, I diligently addressed challenges and iteratively refined the script to realize my objective of extracting and organizing review data from the text file into a neatly formatted XML structure.

-------------------------------------

<div class="alert alert-block alert-warning">

## 7. References <a class="anchor" name="Ref"></a>

</div>

[1]<a class="anchor" name="ref-2"></a> Why do I need to add DOTALL to python regular expression to match new line in raw string, https://stackoverflow.com/questions/22610247, Accessed 30/08/2022.

....

## --------------------------------------------------------------------------------------------------------------------------
"""